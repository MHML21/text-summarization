{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "180d7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/margaretpeterson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The player who played the highest card of the suit led wins the trick, unless a trump is played; then the highest trump card wins the trick. The cards are ranked, in descending order, J (of trump suit), J (same colour as trump suit), A, K, Q, 10, and 9 of the trump suit. A player wishing the proposed suit to be named trump, orders up the card and the dealer adds that card to his or her hand. This expands the spades suit to the seven cards named above and reduces the suit of clubs by one card (its jack being loaned to the trump suit). It is usually more advantageous to the dealer's team to select trump in this way, as the dealer necessarily gains one trump card. If the player bidding and making trump has an exceptionally good hand, that player has the option of playing without their partner and partner's cards. When a suit is named trump, the jack in the suit of the same colour becomes a member of this trump suit.\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import heapq\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ------------------------------------------------- Collect Text Data -------------------------------------------------#\n",
    "scrape_data = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Euchre\")\n",
    "article = scrape_data.read();\n",
    "parsed_article = bs.BeautifulSoup(article, \"lxml\")\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "   # print(p)\n",
    "    article_text += p.text\n",
    "    \n",
    "# ------------------------------------------------- Text Preprocessing ------------------------------------------------- #\n",
    "# Removing square brackets and extra spaces (references in the wikipedia article)\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text) #removes []\n",
    "article_text = re.sub(r'\\s+', ' ', article_text) #removes extra spaces\n",
    "\n",
    "# Removing special characters, digits, punctuation marks\n",
    "formatted_article_text = re.sub(r'[^a-zA-Z]', ' ', article_text )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "\n",
    "# ------------------------------------------------- Tokenize ------------------------------------------------- #\n",
    "# Converting text into separate sentences\n",
    "# Note: use article_text object because it includes punctuation. Otherwise it would be one long string! \n",
    "sentence_list = nltk.sent_tokenize(article_text)\n",
    "\n",
    "# ------------------------------------------------- Weighted Frequency of Occurrence ------------------------------------------------- #\n",
    "# Find weighted frequency of occurrence\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_frequencies = {}\n",
    "# Note: use formatted_article_text objcet because it only includes words. If we used article_text object, it would think punctuation is a word!\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "#normalize the frequencies            \n",
    "max_frequency = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] /= max_frequency\n",
    "\n",
    "# Sum weighted frequencies for each sentence\n",
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30132d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
